# 實驗結果記錄（簡易版）

> 這份文件記錄每次訓練 AI 的結果，用簡單易懂的方式呈現

---

## 結果總覽

| 實驗 | 日期 | 用什麼資料 | AI 模型 | 準確率 | AUC | 會著床的猜對率 | 備註 |
|------|------|-----------|---------|--------|-----|--------------|------|
| EXP-001_example | 2025-01-25 | v1.0 範例 | ResNet18 | 85.7% | 0.72 | 36% | ⚠️ 範例 |
| **EXP-001** | **2025-01-26** | **原始資料** | **ResNet18** | **85.1%** | **0.80** | **76%** | ✅ Recall 最高 |
| **EXP-002** | **2025-01-30** | **原始資料** | **EfficientNet-B0** | **87.5%** | **0.83** | **64%** | ✅ AUC 最高 |
| EXP-003 | 2025-01-30 | 原始資料 | EfficientNet + Focal Loss | 85.7% | 0.80 | 68% | ⚠️ 效果不如預期 |

### 指標說明
- **準確率**：整體猜對多少（但資料不平衡時會騙人！）
- **AUC**：模型分辨能力（0~1，越高越好）
- **會著床的猜對率 (Recall)**：真正會著床的胚胎，AI 猜對幾成 ← **最重要！**

---

## 目前最好的模型

| 排名 | 實驗編號 | AUC | 會著床猜對率 | 備註 |
|------|----------|-----|-------------|------|
| 🥇 1 | EXP-002 | 0.83 | 64% | AUC 最高 |
| 🥈 2 | EXP-001 | 0.80 | 76% | Recall 最高 |
| 3 | | | | |

> ⚠️ **注意**：EXP-001 和 EXP-002 各有優勢，要看你更重視哪個指標！

---

## 詳細實驗記錄

---

### EXP-001_example ⚠️ 這是範例

> 這是展示用的範例，讓大家知道記錄長什麼樣子

**基本資訊**
| 項目 | 內容 |
|------|------|
| 日期 | 2025-01-25 |
| 執行者 | （範例）王小明 |
| 目標 | 建立最基本的模型，看看效果如何 |

**用了什麼資料**
| 項目 | 內容 |
|------|------|
| 資料版本 | v1.0_baseline_example |
| 訓練照片 | 672 張 |
| 驗證照片 | 168 張 |

**AI 設定（簡化版）**
| 設定 | 內容 | 說明 |
|------|------|------|
| 模型 | ResNet18 | 一種常用的圖像辨識 AI |
| 訓練次數 | 50 輪 | AI 看過全部照片 50 遍 |
| 預訓練 | 有 | AI 已經先學過其他照片 |

**結果**

| 指標 | 數值 | 好不好？ |
|------|------|---------|
| 準確率 | 85.7% | 看起來還行，但其實是假象 |
| AUC | 0.72 | 普通，有進步空間 |
| **會著床的猜對率** | **36%** | **不好！很多會著床的沒猜到** |
| 會著床的猜對精準度 | 45% | 不好！猜會著床的常猜錯 |

**預測結果分析**

```
在 168 張驗證照片中：

實際不會著床的 143 張：
  → AI 猜對 135 張 ✓
  → AI 猜錯 8 張 ✗ （誤以為會著床）

實際會著床的 25 張：
  → AI 猜對 9 張 ✓
  → AI 猜錯 16 張 ✗ （漏掉了！這是大問題）
```

**學到什麼**

1. **大問題**：AI 太保守了
   - 幾乎都猜「不會著床」
   - 因為訓練資料裡 85% 都是不會著床的

2. **為什麼準確率會騙人**
   - 如果 AI 全部猜「不會著床」，準確率也有 85%
   - 但這樣完全沒用！

3. **下次要改進的方向**
   - 讓 AI 更重視「會著床」的照片
   - 或是增加「會著床」照片的數量

---

### EXP-001：第一個基準模型 ✅

**基本資訊**
| 項目 | 內容 |
|------|------|
| 日期 | 2025-01-26 |
| 執行者 | 田曜瑞 |
| 目標 | 建立第一個基準模型，了解資料特性 |
| 執行環境 | Google Colab (T4 GPU) |

**用了什麼資料**
| 項目 | 內容 |
|------|------|
| 資料版本 | 原始資料 (hvwc23) |
| 訓練照片 | 672 張 (80%) |
| 驗證照片 | 168 張 (20%) |

**AI 設定**
| 設定 | 內容 | 說明 |
|------|------|------|
| 模型 | ResNet18 | 常用的圖像辨識 AI |
| 訓練次數 | 20 輪 | AI 看過全部照片 20 遍 |
| 預訓練 | 有 | AI 已經先學過 ImageNet 照片 |
| 類別加權 | 有 | 讓 AI 更重視「會著床」的照片 |

**結果**

| 指標 | 數值 | 好不好？ |
|------|------|---------|
| 準確率 | 85.1% | 還行 |
| AUC | **0.80** | 👍 不錯！接近 0.8 |
| **會著床的猜對率** | **76%** | 👍 **很好！比範例的 36% 好很多** |
| 會著床的猜對精準度 | 50% | ⚠️ 普通，一半是誤報 |

**白話解釋結果**

```
假設有 100 個胚胎，其中 25 個真的會著床：

AI 的表現：
✅ 猜對會著床的：19 個（76% 的 25 個）
❌ 漏掉會著床的：6 個
⚠️ 誤報（不會卻說會）：約 19 個

→ 好消息：漏掉的不多！
→ 待改進：誤報有點多
```

**學到什麼**

1. **類別加權有效！**
   - 加了類別加權後，「會著床的猜對率」從範例的 36% 提升到 76%
   - 這是處理資料不平衡的好方法

2. **Precision 和 Recall 的取捨**
   - 猜對率高（76%），但精準度只有 50%
   - 這代表 AI 比較「大膽」，寧可多猜一些

3. **下次可以試的方向**
   - 調整判斷閾值，看能不能減少誤報
   - 試試其他模型（EfficientNet）

---

### EXP-002：換用 EfficientNet 模型 ✅

**基本資訊**
| 項目 | 內容 |
|------|------|
| 日期 | 2025-01-30 |
| 執行者 | 田曜瑞 |
| 目標 | 測試 EfficientNet-B0 是否比 ResNet18 好 |
| 執行環境 | Google Colab (T4 GPU) |

**用了什麼資料**
| 項目 | 內容 |
|------|------|
| 資料版本 | 原始資料 (hvwc23) |
| 訓練照片 | 672 張 (80%) |
| 驗證照片 | 168 張 (20%) |

**AI 設定**
| 設定 | 內容 | 與 EXP-001 差異 |
|------|------|----------------|
| 模型 | EfficientNet-B0 | 🔄 不同 |
| 訓練次數 | 30 輪 | 相同 |
| 預訓練 | 有 | 相同 |
| 類別加權 | 有 | 相同 |

**結果**

| 指標 | 數值 | 比 EXP-001？ |
|------|------|-------------|
| 準確率 | 87.5% | ✅ +2.4% |
| AUC | **0.83** | ✅ **+0.03** |
| **會著床的猜對率** | **64%** | ❌ **-12%** |
| 會著床的猜對精準度 | 57% | ✅ +7% |

**混淆矩陣**
```
在 168 張驗證照片中：

實際不會著床的 143 張：
  → AI 猜對 131 張 ✓
  → AI 猜錯 12 張 ✗ （誤報減少了！）

實際會著床的 25 張：
  → AI 猜對 16 張 ✓
  → AI 猜錯 9 張 ✗ （漏掉變多了！）
```

**白話解釋**

> EfficientNet 比較「謹慎」：
> - 它說「會著床」時更準（57% vs 50%）
> - 但它不敢猜，所以漏掉的變多（9 個 vs 6 個）
>
> ResNet18 比較「大膽」：
> - 它說「會著床」時常猜錯（50%）
> - 但它敢猜，所以漏掉的較少（6 個）

**學到什麼**

1. **AUC 提升，但 Recall 下降**
   - 這是典型的 trade-off（權衡）
   - 模型越謹慎 → Precision 高、Recall 低
   - 模型越大膽 → Precision 低、Recall 高

2. **哪個模型比較好？看你的目標**
   - 如果「漏掉好胚胎」代價很高 → 選 EXP-001 (Recall 76%)
   - 如果「誤報」代價很高 → 選 EXP-002 (Precision 57%)

3. **下次可以試的方向**
   - 調整 EfficientNet 的判斷閾值，讓它更「大膽」
   - 使用 Focal Loss 替代類別加權

---

### EXP-003：嘗試 Focal Loss ⚠️

**基本資訊**
| 項目 | 內容 |
|------|------|
| 日期 | 2025-01-30 |
| 執行者 | 田曜瑞 |
| 目標 | 測試 Focal Loss 是否能同時提升 AUC 和 Recall |
| 執行環境 | Google Colab (T4 GPU) |

**用了什麼資料**
| 項目 | 內容 |
|------|------|
| 資料版本 | 原始資料 (hvwc23) |
| 訓練照片 | 672 張 (80%) |
| 驗證照片 | 168 張 (20%) |

**AI 設定**
| 設定 | 內容 | 與 EXP-002 差異 |
|------|------|----------------|
| 模型 | EfficientNet-B0 | 相同 |
| 訓練次數 | 30 輪 | 相同 |
| 損失函數 | Focal Loss (gamma=2) | 🔄 不同 |
| 類別加權 | 有 | 相同 |

**結果**

| 指標 | 數值 | 比 EXP-002？ |
|------|------|-------------|
| 準確率 | 85.7% | ❌ -1.8% |
| AUC | 0.80 | ❌ -0.03 |
| **會著床的猜對率** | **68%** | ✅ **+4%** |
| 會著床的猜對精準度 | 51.5% | ❌ -5.5% |

**混淆矩陣**
```
在 168 張驗證照片中：

實際不會著床的 143 張：
  → AI 猜對 127 張 ✓
  → AI 猜錯 16 張 ✗ （誤報變多了）

實際會著床的 25 張：
  → AI 猜對 17 張 ✓
  → AI 猜錯 8 張 ✗ （漏掉比 EXP-002 少）
```

**三次實驗比較**

| 實驗 | 模型 | 損失函數 | AUC | Recall | Precision |
|------|------|----------|-----|--------|-----------|
| EXP-001 | ResNet18 | CE+加權 | 0.80 | **76%** | 50% |
| EXP-002 | EfficientNet | CE+加權 | **0.83** | 64% | **57%** |
| EXP-003 | EfficientNet | Focal Loss | 0.80 | 68% | 51.5% |

**學到什麼**

1. **Focal Loss 在這個案例效果不如預期**
   - Recall 小幅提升（64% → 68%），但 AUC 反而下降（0.83 → 0.80）
   - 沒有達到「兩全其美」的效果

2. **可能的原因**
   - 資料量太少（只有 672 張），Focal Loss 的優勢沒發揮出來
   - gamma=2 可能不是最佳值

3. **結論**
   - 對於這個資料集，**類別加權 (Weighted CE) 比 Focal Loss 效果好**
   - 目前最佳模型仍是 EXP-001（Recall 最高）或 EXP-002（AUC 最高）

---

## 實驗心得總結

### 有效的方法
（記錄哪些嘗試讓結果變好）
- ✅ **類別加權 (Weighted Loss)**：讓 AI 更重視少數類，Recall 從 36% → 76%

### 沒效的方法
（記錄哪些嘗試沒用，避免重複踩坑）
- ⚠️ **Focal Loss (EXP-003)**：在這個小資料集上，效果不如簡單的類別加權

### 重要發現
- 資料不平衡（85% vs 15%）是大問題，一定要處理
- 準確率會騙人！要看 AUC 和 Recall
- 在醫療情境，**漏掉好胚胎比誤報更嚴重**，所以 Recall 最重要
