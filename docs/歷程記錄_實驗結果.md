# 實驗結果記錄（簡易版）

> 這份文件記錄每次訓練 AI 的結果，用簡單易懂的方式呈現

---

## 結果總覽

| 實驗 | 日期 | 用什麼資料 | AI 模型 | 準確率 | AUC | 會著床的猜對率 | 備註 |
|------|------|-----------|---------|--------|-----|--------------|------|
| EXP-001_example | 2025-01-25 | v1.0 範例 | ResNet18 | 85.7% | 0.72 | 36% | ⚠️ 範例 |
| **EXP-001** | **2025-01-26** | **原始資料** | **ResNet18** | **85.1%** | **0.80** | **76%** | ✅ 基準模型 |
| EXP-002 | | | | | | | 待執行 |

### 指標說明
- **準確率**：整體猜對多少（但資料不平衡時會騙人！）
- **AUC**：模型分辨能力（0~1，越高越好）
- **會著床的猜對率 (Recall)**：真正會著床的胚胎，AI 猜對幾成 ← **最重要！**

---

## 目前最好的模型

| 排名 | 實驗編號 | AUC | 會著床猜對率 | 備註 |
|------|----------|-----|-------------|------|
| 🥇 1 | EXP-001 | 0.80 | 76% | 目前最佳 |
| 2 | | | | |
| 3 | | | | |

---

## 詳細實驗記錄

---

### EXP-001_example ⚠️ 這是範例

> 這是展示用的範例，讓大家知道記錄長什麼樣子

**基本資訊**
| 項目 | 內容 |
|------|------|
| 日期 | 2025-01-25 |
| 執行者 | （範例）王小明 |
| 目標 | 建立最基本的模型，看看效果如何 |

**用了什麼資料**
| 項目 | 內容 |
|------|------|
| 資料版本 | v1.0_baseline_example |
| 訓練照片 | 672 張 |
| 驗證照片 | 168 張 |

**AI 設定（簡化版）**
| 設定 | 內容 | 說明 |
|------|------|------|
| 模型 | ResNet18 | 一種常用的圖像辨識 AI |
| 訓練次數 | 50 輪 | AI 看過全部照片 50 遍 |
| 預訓練 | 有 | AI 已經先學過其他照片 |

**結果**

| 指標 | 數值 | 好不好？ |
|------|------|---------|
| 準確率 | 85.7% | 看起來還行，但其實是假象 |
| AUC | 0.72 | 普通，有進步空間 |
| **會著床的猜對率** | **36%** | **不好！很多會著床的沒猜到** |
| 會著床的猜對精準度 | 45% | 不好！猜會著床的常猜錯 |

**預測結果分析**

```
在 168 張驗證照片中：

實際不會著床的 143 張：
  → AI 猜對 135 張 ✓
  → AI 猜錯 8 張 ✗ （誤以為會著床）

實際會著床的 25 張：
  → AI 猜對 9 張 ✓
  → AI 猜錯 16 張 ✗ （漏掉了！這是大問題）
```

**學到什麼**

1. **大問題**：AI 太保守了
   - 幾乎都猜「不會著床」
   - 因為訓練資料裡 85% 都是不會著床的

2. **為什麼準確率會騙人**
   - 如果 AI 全部猜「不會著床」，準確率也有 85%
   - 但這樣完全沒用！

3. **下次要改進的方向**
   - 讓 AI 更重視「會著床」的照片
   - 或是增加「會著床」照片的數量

---

### EXP-001：第一個基準模型 ✅

**基本資訊**
| 項目 | 內容 |
|------|------|
| 日期 | 2025-01-26 |
| 執行者 | 田曜瑞 |
| 目標 | 建立第一個基準模型，了解資料特性 |
| 執行環境 | Google Colab (T4 GPU) |

**用了什麼資料**
| 項目 | 內容 |
|------|------|
| 資料版本 | 原始資料 (hvwc23) |
| 訓練照片 | 672 張 (80%) |
| 驗證照片 | 168 張 (20%) |

**AI 設定**
| 設定 | 內容 | 說明 |
|------|------|------|
| 模型 | ResNet18 | 常用的圖像辨識 AI |
| 訓練次數 | 20 輪 | AI 看過全部照片 20 遍 |
| 預訓練 | 有 | AI 已經先學過 ImageNet 照片 |
| 類別加權 | 有 | 讓 AI 更重視「會著床」的照片 |

**結果**

| 指標 | 數值 | 好不好？ |
|------|------|---------|
| 準確率 | 85.1% | 還行 |
| AUC | **0.80** | 👍 不錯！接近 0.8 |
| **會著床的猜對率** | **76%** | 👍 **很好！比範例的 36% 好很多** |
| 會著床的猜對精準度 | 50% | ⚠️ 普通，一半是誤報 |

**白話解釋結果**

```
假設有 100 個胚胎，其中 25 個真的會著床：

AI 的表現：
✅ 猜對會著床的：19 個（76% 的 25 個）
❌ 漏掉會著床的：6 個
⚠️ 誤報（不會卻說會）：約 19 個

→ 好消息：漏掉的不多！
→ 待改進：誤報有點多
```

**學到什麼**

1. **類別加權有效！**
   - 加了類別加權後，「會著床的猜對率」從範例的 36% 提升到 76%
   - 這是處理資料不平衡的好方法

2. **Precision 和 Recall 的取捨**
   - 猜對率高（76%），但精準度只有 50%
   - 這代表 AI 比較「大膽」，寧可多猜一些

3. **下次可以試的方向**
   - 調整判斷閾值，看能不能減少誤報
   - 試試其他模型（EfficientNet）

---

### EXP-002：（待執行）

**基本資訊**
| 項目 | 內容 |
|------|------|
| 日期 | |
| 執行者 | |
| 目標 | |

**結果**
| 指標 | 數值 | 比 EXP-001 好嗎？ |
|------|------|------------------|
| AUC | | |
| 會著床的猜對率 | | |

---

## 實驗心得總結

### 有效的方法
（記錄哪些嘗試讓結果變好）
- ✅ **類別加權 (Weighted Loss)**：讓 AI 更重視少數類，Recall 從 36% → 76%

### 沒效的方法
（記錄哪些嘗試沒用，避免重複踩坑）
- （待補充）

### 重要發現
- 資料不平衡（85% vs 15%）是大問題，一定要處理
- 準確率會騙人！要看 AUC 和 Recall
- 在醫療情境，**漏掉好胚胎比誤報更嚴重**，所以 Recall 最重要
