{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¬ èƒšèƒè‘—åºŠé æ¸¬ - Focal Loss å¯¦é©—\n",
    "\n",
    "> EXP-003: ä½¿ç”¨ Focal Loss è™•ç†é¡åˆ¥ä¸å¹³è¡¡\n",
    "\n",
    "**ç›®æ¨™**ï¼šæ¸¬è©¦ Focal Loss æ˜¯å¦èƒ½åŒæ™‚ä¿æŒé«˜ AUC å’Œæå‡ Recall\n",
    "\n",
    "**æ”¹å‹•**ï¼šå°‡ CrossEntropyLoss + é¡åˆ¥åŠ æ¬Š æ”¹ç‚º Focal Loss\n",
    "\n",
    "---\n",
    "\n",
    "## Focal Loss æ˜¯ä»€éº¼ï¼Ÿ\n",
    "\n",
    "**ç™½è©±è§£é‡‹**ï¼š\n",
    "- åŸæœ¬çš„ Lossï¼šä¸ç®¡ AI çŒœå¾—å¤šæœ‰ä¿¡å¿ƒï¼ŒéŒ¯äº†å°±ç½°ä¸€æ¨£å¤š\n",
    "- Focal Lossï¼šAI å·²ç¶“å¾ˆæœ‰ä¿¡å¿ƒä¸”çŒœå°çš„ï¼Œå°‘ç½°ä¸€é»ï¼›AI ä¸ç¢ºå®šçš„ï¼Œå¤šç½°ä¸€é»\n",
    "\n",
    "**æ•ˆæœ**ï¼šè®“ AI å°ˆæ³¨åœ¨ã€Œé›£åˆ†è¾¨ã€çš„æ¨£æœ¬ä¸Šï¼Œè€Œä¸æ˜¯ä¸€ç›´å­¸ç°¡å–®çš„\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ç’°å¢ƒè¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ›è¼‰ Google Drive\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"âœ… Google Drive å·²æ›è¼‰\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ ä¸åœ¨ Colab ç’°å¢ƒï¼Œè·³éæ›è¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒ¯å…¥å¥—ä»¶\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, \n",
    "    recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# æª¢æŸ¥ GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸ ä½¿ç”¨è£ç½®: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ è¨­å®šè³‡æ–™è·¯å¾‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    DATA_ROOT = '/content/drive/MyDrive/hvwc23'\n",
    "else:\n",
    "    DATA_ROOT = '../data/raw/hvwc23'\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, 'train')\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, 'train.csv')\n",
    "\n",
    "print(\"ğŸ“ æª¢æŸ¥è³‡æ–™è·¯å¾‘...\")\n",
    "print(f\"   è¨“ç·´è³‡æ–™å¤¾: {os.path.exists(TRAIN_DIR)}\")\n",
    "print(f\"   è¨“ç·´ CSV:   {os.path.exists(TRAIN_CSV)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ è¼‰å…¥èˆ‡æº–å‚™è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥è³‡æ–™\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "print(f\"ğŸ“Š è¨“ç·´è³‡æ–™ç­†æ•¸: {len(train_df)}\")\n",
    "\n",
    "# åˆ†å‰²\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, test_size=0.2, random_state=SEED, stratify=train_df['Class']\n",
    ")\n",
    "print(f\"   è¨“ç·´é›†: {len(train_data)} å¼µ\")\n",
    "print(f\"   é©—è­‰é›†: {len(val_data)} å¼µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è³‡æ–™è½‰æ›ï¼ˆèˆ‡ä¹‹å‰ç›¸åŒï¼‰\n",
    "IMG_SIZE = 224\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"âœ… è³‡æ–™è½‰æ›å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class EmbryoDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['Image'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = row['Class']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "train_dataset = EmbryoDataset(train_data, TRAIN_DIR, train_transform)\n",
    "val_dataset = EmbryoDataset(val_data, TRAIN_DIR, val_transform)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"âœ… DataLoader å»ºç«‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ å®šç¾© Focal Loss\n",
    "\n",
    "ğŸ”„ **é€™æ˜¯èˆ‡ EXP-001ã€EXP-002 çš„ä¸»è¦å·®ç•°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# ğŸ”„ Focal Loss å®šç¾©\n",
    "# =============================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss - è®“æ¨¡å‹å°ˆæ³¨åœ¨é›£åˆ†é¡çš„æ¨£æœ¬\n",
    "    \n",
    "    åƒæ•¸ï¼š\n",
    "    - alpha: é¡åˆ¥æ¬Šé‡ï¼ˆè™•ç†ä¸å¹³è¡¡ï¼‰\n",
    "    - gamma: èšç„¦ç¨‹åº¦ï¼ˆè¶Šå¤§è¶Šå°ˆæ³¨åœ¨é›£çš„æ¨£æœ¬ï¼‰\n",
    "    \n",
    "    å…¬å¼ï¼šFL = -alpha * (1-p)^gamma * log(p)\n",
    "    - ç•¶ p æ¥è¿‘ 1ï¼ˆå¾ˆæœ‰ä¿¡å¿ƒä¸”çŒœå°ï¼‰ï¼š(1-p)^gamma å¾ˆå°ï¼Œloss è®Šå°\n",
    "    - ç•¶ p æ¥è¿‘ 0ï¼ˆå¾ˆæ²’ä¿¡å¿ƒæˆ–çŒœéŒ¯ï¼‰ï¼š(1-p)^gamma æ¥è¿‘ 1ï¼Œloss ç¶­æŒ\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # é¡åˆ¥æ¬Šé‡\n",
    "        self.gamma = gamma  # èšç„¦åƒæ•¸\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # è¨ˆç®—åŸºæœ¬çš„ CrossEntropy\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # è¨ˆç®—é æ¸¬æ©Ÿç‡\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # è¨ˆç®— Focal æ¬Šé‡\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # åŠ å…¥é¡åˆ¥æ¬Šé‡\n",
    "        if self.alpha is not None:\n",
    "            alpha_weight = self.alpha[targets]\n",
    "            focal_loss = alpha_weight * focal_weight * ce_loss\n",
    "        else:\n",
    "            focal_loss = focal_weight * ce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "print(\"âœ… Focal Loss å®šç¾©å®Œæˆ\")\n",
    "print()\n",
    "print(\"ğŸ“Š Focal Loss vs CrossEntropyLoss å·®ç•°ï¼š\")\n",
    "print(\"   CrossEntropyLoss: æ‰€æœ‰æ¨£æœ¬ä¸€è¦–åŒä»\")\n",
    "print(\"   Focal Loss: å°ˆæ³¨åœ¨é›£åˆ†é¡çš„æ¨£æœ¬ï¼ˆgamma è¶Šå¤§è¶Šå°ˆæ³¨ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ å»ºç«‹æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ EfficientNet-B0ï¼ˆå› ç‚º AUC è¼ƒé«˜ï¼‰\n",
    "model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_features, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"âœ… æ¨¡å‹å»ºç«‹å®Œæˆ: EfficientNet-B0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—é¡åˆ¥æ¬Šé‡ï¼ˆç”¨æ–¼ Focal Lossï¼‰\n",
    "class_counts = train_data['Class'].value_counts().sort_index()\n",
    "total = len(train_data)\n",
    "alpha = torch.tensor([total / (2 * class_counts[i]) for i in range(2)], dtype=torch.float32)\n",
    "alpha = alpha.to(device)\n",
    "\n",
    "print(f\"ğŸ“Š é¡åˆ¥æ¬Šé‡ (alpha):\")\n",
    "print(f\"   Class 0: {alpha[0]:.4f}\")\n",
    "print(f\"   Class 1: {alpha[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# ğŸ”„ ä½¿ç”¨ Focal Lossï¼ˆèˆ‡ EXP-001ã€EXP-002 ä¸åŒï¼‰\n",
    "# =============================================\n",
    "\n",
    "# Focal Loss åƒæ•¸\n",
    "GAMMA = 2.0  # èšç„¦ç¨‹åº¦ï¼Œé€šå¸¸ç”¨ 2\n",
    "\n",
    "criterion = FocalLoss(alpha=alpha, gamma=GAMMA)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "print(\"âœ… è¨“ç·´è¨­å®šå®Œæˆ\")\n",
    "print(f\"   æå¤±å‡½æ•¸: Focal Loss (gamma={GAMMA})\")\n",
    "print(f\"   å„ªåŒ–å™¨: Adam (lr=0.001)\")\n",
    "print()\n",
    "print(\"ğŸ“Š èˆ‡ä¹‹å‰å¯¦é©—çš„å·®ç•°ï¼š\")\n",
    "print(\"   EXP-001: CrossEntropyLoss + é¡åˆ¥åŠ æ¬Š\")\n",
    "print(\"   EXP-002: CrossEntropyLoss + é¡åˆ¥åŠ æ¬Š\")\n",
    "print(\"   EXP-003: Focal Loss + é¡åˆ¥åŠ æ¬Š â† æ–°çš„ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ è¨“ç·´æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´å‡½æ•¸\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), correct / total\n",
    "\n",
    "# é©—è­‰å‡½æ•¸\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    return running_loss / len(loader), accuracy, auc, all_preds, all_probs, all_labels\n",
    "\n",
    "print(\"âœ… è¨“ç·´å‡½æ•¸å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é–‹å§‹è¨“ç·´\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_auc': []\n",
    "}\n",
    "\n",
    "best_auc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ é–‹å§‹è¨“ç·´ (Focal Loss)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, val_auc, _, _, _ = validate(model, val_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    \n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), 'best_model_focal.pth')\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… è¨“ç·´å®Œæˆï¼æœ€ä½³ AUC: {best_auc:.4f} (Epoch {best_epoch})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ è¦–è¦ºåŒ–è¨“ç·´éç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train', marker='o', markersize=3)\n",
    "axes[0].plot(history['val_loss'], label='Validation', marker='o', markersize=3)\n",
    "axes[0].set_title('Loss (Focal Loss)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['train_acc'], label='Train', marker='o', markersize=3)\n",
    "axes[1].plot(history['val_acc'], label='Validation', marker='o', markersize=3)\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history['val_auc'], label='EXP-003 (Focal)', marker='o', markersize=3, color='green')\n",
    "axes[2].axhline(y=best_auc, color='green', linestyle='--', alpha=0.5)\n",
    "axes[2].axhline(y=0.7986, color='blue', linestyle=':', label='EXP-001: 0.7986')\n",
    "axes[2].axhline(y=0.8283, color='orange', linestyle=':', label='EXP-002: 0.8283')\n",
    "axes[2].set_title('Validation AUC')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves_focal.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ æœ€çµ‚è©•ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥æœ€ä½³æ¨¡å‹\n",
    "model.load_state_dict(torch.load('best_model_focal.pth'))\n",
    "print(f\"âœ… è¼‰å…¥æœ€ä½³æ¨¡å‹ (Epoch {best_epoch})\")\n",
    "\n",
    "# æœ€çµ‚é©—è­‰\n",
    "val_loss, val_acc, val_auc, all_preds, all_probs, all_labels = validate(\n",
    "    model, val_loader, criterion, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—è©³ç´°æŒ‡æ¨™\n",
    "precision_1 = precision_score(all_labels, all_preds, pos_label=1)\n",
    "recall_1 = recall_score(all_labels, all_preds, pos_label=1)\n",
    "f1_1 = f1_score(all_labels, all_preds, pos_label=1)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š EXP-003 æœ€çµ‚è©•ä¼°çµæœ\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"âœ… Accuracy:  {val_acc:.4f} ({val_acc*100:.1f}%)\")\n",
    "print(f\"âœ… AUC-ROC:   {val_auc:.4f}\")\n",
    "print(f\"âœ… Precision: {precision_1:.4f}\")\n",
    "print(f\"âœ… Recall:    {recall_1:.4f}\")\n",
    "print(f\"âœ… F1-Score:  {f1_1:.4f}\")\n",
    "print()\n",
    "print(\"ğŸ“Š ä¸‰æ¬¡å¯¦é©—æ¯”è¼ƒï¼š\")\n",
    "print(f\"{'å¯¦é©—':<10} {'æ¨¡å‹':<15} {'æå¤±å‡½æ•¸':<20} {'AUC':<8} {'Recall':<8} {'Precision':<8}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'EXP-001':<10} {'ResNet18':<15} {'CE + åŠ æ¬Š':<20} {'0.7986':<8} {'0.7600':<8} {'0.5000':<8}\")\n",
    "print(f\"{'EXP-002':<10} {'EfficientNet':<15} {'CE + åŠ æ¬Š':<20} {'0.8283':<8} {'0.6400':<8} {'0.5714':<8}\")\n",
    "print(f\"{'EXP-003':<10} {'EfficientNet':<15} {'Focal Loss':<20} {val_auc:<8.4f} {recall_1:<8.4f} {precision_1:<8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ··æ·†çŸ©é™£\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['é æ¸¬: ä¸è‘—åºŠ', 'é æ¸¬: è‘—åºŠ'],\n",
    "            yticklabels=['å¯¦éš›: ä¸è‘—åºŠ', 'å¯¦éš›: è‘—åºŠ'])\n",
    "plt.title('æ··æ·†çŸ©é™£ - Focal Loss (EXP-003)')\n",
    "plt.ylabel('å¯¦éš›å€¼')\n",
    "plt.xlabel('é æ¸¬å€¼')\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "plt.figtext(0.5, -0.05, \n",
    "            f'TN={tn} | FP={fp} | FN={fn} | TP={tp}', \n",
    "            ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_focal.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ çµæœæ‘˜è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸ“‹ EXP-003 çµæœæ‘˜è¦ï¼ˆè«‹è¤‡è£½æ­¤å€å¡Šå›å ±ï¼‰\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "## å¯¦é©—è¨­å®š\n",
    "- æ¨¡å‹: EfficientNet-B0 (é è¨“ç·´)\n",
    "- æå¤±å‡½æ•¸: Focal Loss (gamma={GAMMA})\n",
    "- è¨“ç·´é›†: {len(train_data)} å¼µ\n",
    "- é©—è­‰é›†: {len(val_data)} å¼µ\n",
    "- Batch size: {BATCH_SIZE}\n",
    "- Epochs: {NUM_EPOCHS}\n",
    "\n",
    "## çµæœ\n",
    "| æŒ‡æ¨™ | æ•¸å€¼ |\n",
    "|------|------|\n",
    "| Accuracy | {val_acc:.4f} ({val_acc*100:.1f}%) |\n",
    "| AUC-ROC | {val_auc:.4f} |\n",
    "| Precision (è‘—åºŠ) | {precision_1:.4f} |\n",
    "| Recall (è‘—åºŠ) | {recall_1:.4f} |\n",
    "| F1-Score (è‘—åºŠ) | {f1_1:.4f} |\n",
    "\n",
    "## æ··æ·†çŸ©é™£\n",
    "- TN (çŒœå°ä¸è‘—åºŠ): {tn}\n",
    "- FP (èª¤å ±): {fp}\n",
    "- FN (æ¼æ‰): {fn}\n",
    "- TP (çŒœå°è‘—åºŠ): {tp}\n",
    "\n",
    "## ä¸‰æ¬¡å¯¦é©—æ¯”è¼ƒ\n",
    "| å¯¦é©— | æ¨¡å‹ | æå¤±å‡½æ•¸ | AUC | Recall | Precision |\n",
    "|------|------|----------|-----|--------|----------|\n",
    "| EXP-001 | ResNet18 | CE+åŠ æ¬Š | 0.7986 | 0.7600 | 0.5000 |\n",
    "| EXP-002 | EfficientNet | CE+åŠ æ¬Š | 0.8283 | 0.6400 | 0.5714 |\n",
    "| EXP-003 | EfficientNet | Focal Loss | {val_auc:.4f} | {recall_1:.4f} | {precision_1:.4f} |\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
