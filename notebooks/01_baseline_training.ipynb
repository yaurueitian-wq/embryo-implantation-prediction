{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¬ èƒšèƒè‘—åºŠé æ¸¬ - Baseline æ¨¡å‹è¨“ç·´\n",
    "\n",
    "> EXP-001: ä½¿ç”¨ ResNet18 å»ºç«‹åŸºæº–æ¨¡å‹\n",
    "\n",
    "**ç›®æ¨™**ï¼šå»ºç«‹ç¬¬ä¸€å€‹åŸºæº–æ¨¡å‹ï¼Œäº†è§£è³‡æ–™ç‰¹æ€§èˆ‡åŸºæœ¬æ•ˆèƒ½\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ç’°å¢ƒè¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ›è¼‰ Google Driveï¼ˆå¦‚æœåœ¨ Colab ä¸ŠåŸ·è¡Œï¼‰\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"âœ… Google Drive å·²æ›è¼‰\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ ä¸åœ¨ Colab ç’°å¢ƒï¼Œè·³éæ›è¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶ï¼ˆColab é€šå¸¸å·²é è£ï¼‰\n",
    "# !pip install torch torchvision scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒ¯å…¥å¥—ä»¶\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, \n",
    "    recall_score, f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# è¨­å®šéš¨æ©Ÿç¨®å­ï¼ˆç¢ºä¿å¯é‡ç¾ï¼‰\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# æª¢æŸ¥ GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸ ä½¿ç”¨è£ç½®: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ è¨­å®šè³‡æ–™è·¯å¾‘\n",
    "\n",
    "âš ï¸ **è«‹æ ¹æ“šä½ çš„è³‡æ–™ä½ç½®ä¿®æ”¹ä¸‹æ–¹è·¯å¾‘**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# âš ï¸ è«‹ä¿®æ”¹é€™è£¡çš„è·¯å¾‘ï¼\n",
    "# =============================================\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Colabï¼šè³‡æ–™åœ¨ Google Drive ä¸­\n",
    "    DATA_ROOT = '/content/drive/MyDrive/hvwc23'  # â† ä¿®æ”¹æˆä½ çš„è·¯å¾‘\n",
    "else:\n",
    "    # æœ¬æ©Ÿ\n",
    "    DATA_ROOT = '../data/raw/hvwc23'\n",
    "\n",
    "# ç¢ºèªè·¯å¾‘\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, 'train')\n",
    "TEST_DIR = os.path.join(DATA_ROOT, 'test')\n",
    "TRAIN_CSV = os.path.join(DATA_ROOT, 'train.csv')\n",
    "TEST_CSV = os.path.join(DATA_ROOT, 'test.csv')\n",
    "\n",
    "# æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n",
    "print(\"ğŸ“ æª¢æŸ¥è³‡æ–™è·¯å¾‘...\")\n",
    "print(f\"   è¨“ç·´è³‡æ–™å¤¾: {os.path.exists(TRAIN_DIR)} - {TRAIN_DIR}\")\n",
    "print(f\"   æ¸¬è©¦è³‡æ–™å¤¾: {os.path.exists(TEST_DIR)} - {TEST_DIR}\")\n",
    "print(f\"   è¨“ç·´ CSV:   {os.path.exists(TRAIN_CSV)} - {TRAIN_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ è¼‰å…¥èˆ‡æ¢ç´¢è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥æ¨™ç±¤\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "print(f\"ğŸ“Š è¨“ç·´è³‡æ–™ç­†æ•¸: {len(train_df)}\")\n",
    "print()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡åˆ¥åˆ†ä½ˆ\n",
    "print(\"ğŸ“Š é¡åˆ¥åˆ†ä½ˆ:\")\n",
    "class_counts = train_df['Class'].value_counts().sort_index()\n",
    "for cls, count in class_counts.items():\n",
    "    pct = count / len(train_df) * 100\n",
    "    label = \"ä¸æœƒè‘—åºŠ\" if cls == 0 else \"æœƒè‘—åºŠ\"\n",
    "    print(f\"   Class {cls} ({label}): {count} å¼µ ({pct:.1f}%)\")\n",
    "\n",
    "# è¦–è¦ºåŒ–\n",
    "plt.figure(figsize=(8, 4))\n",
    "colors = ['#ff6b6b', '#4ecdc4']\n",
    "plt.bar(['Class 0\\n(ä¸æœƒè‘—åºŠ)', 'Class 1\\n(æœƒè‘—åºŠ)'], class_counts.values, color=colors)\n",
    "plt.title('é¡åˆ¥åˆ†ä½ˆï¼ˆåš´é‡ä¸å¹³è¡¡ï¼ï¼‰', fontsize=14)\n",
    "plt.ylabel('æ•¸é‡')\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    plt.text(i, v + 10, f'{v}', ha='center', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹ä¸€äº›æ¨£æœ¬åœ–ç‰‡\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# Class 0 æ¨£æœ¬\n",
    "class0_samples = train_df[train_df['Class'] == 0].sample(5)\n",
    "for i, (_, row) in enumerate(class0_samples.iterrows()):\n",
    "    img_path = os.path.join(TRAIN_DIR, row['Image'])\n",
    "    img = Image.open(img_path)\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f\"Class 0\\n{row['Image']}\", fontsize=9)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Class 1 æ¨£æœ¬\n",
    "class1_samples = train_df[train_df['Class'] == 1].sample(5)\n",
    "for i, (_, row) in enumerate(class1_samples.iterrows()):\n",
    "    img_path = os.path.join(TRAIN_DIR, row['Image'])\n",
    "    img = Image.open(img_path)\n",
    "    axes[1, i].imshow(img)\n",
    "    axes[1, i].set_title(f\"Class 1\\n{row['Image']}\", fontsize=9)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('æ¨£æœ¬åœ–ç‰‡ï¼ˆä¸Šæ’: ä¸æœƒè‘—åºŠ, ä¸‹æ’: æœƒè‘—åºŠï¼‰', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ è³‡æ–™å‰è™•ç†èˆ‡åˆ†å‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†ï¼ˆ80/20ï¼Œåˆ†å±¤æŠ½æ¨£ï¼‰\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.2, \n",
    "    random_state=SEED, \n",
    "    stratify=train_df['Class']  # ç¢ºä¿é¡åˆ¥æ¯”ä¾‹ä¸€è‡´\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š è³‡æ–™åˆ†å‰²çµæœ:\")\n",
    "print(f\"   è¨“ç·´é›†: {len(train_data)} å¼µ\")\n",
    "print(f\"   é©—è­‰é›†: {len(val_data)} å¼µ\")\n",
    "print()\n",
    "print(f\"   è¨“ç·´é›†é¡åˆ¥åˆ†ä½ˆ: {dict(train_data['Class'].value_counts().sort_index())}\")\n",
    "print(f\"   é©—è­‰é›†é¡åˆ¥åˆ†ä½ˆ: {dict(val_data['Class'].value_counts().sort_index())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©è³‡æ–™å¢å¼·èˆ‡è½‰æ›\n",
    "IMG_SIZE = 224  # ResNet æ¨™æº–è¼¸å…¥å¤§å°\n",
    "\n",
    "# è¨“ç·´é›†ï¼šæœ‰è³‡æ–™å¢å¼·\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet æ¨™æº–\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# é©—è­‰é›†ï¼šä¸åšå¢å¼·\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"âœ… è³‡æ–™è½‰æ›å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå®šç¾© Dataset\n",
    "class EmbryoDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['Image'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = row['Class']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# å»ºç«‹ Dataset\n",
    "train_dataset = EmbryoDataset(train_data, TRAIN_DIR, train_transform)\n",
    "val_dataset = EmbryoDataset(val_data, TRAIN_DIR, val_transform)\n",
    "\n",
    "print(f\"âœ… Dataset å»ºç«‹å®Œæˆ\")\n",
    "print(f\"   è¨“ç·´é›†: {len(train_dataset)} å¼µ\")\n",
    "print(f\"   é©—è­‰é›†: {len(val_dataset)} å¼µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… DataLoader å»ºç«‹å®Œæˆ\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   è¨“ç·´ batches: {len(train_loader)}\")\n",
    "print(f\"   é©—è­‰ batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ å»ºç«‹æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨é è¨“ç·´çš„ ResNet18\n",
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# ä¿®æ”¹æœ€å¾Œä¸€å±¤ï¼ˆåŸæœ¬æ˜¯ 1000 é¡ï¼Œæ”¹æˆ 2 é¡ï¼‰\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# ç§»åˆ° GPU\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"âœ… æ¨¡å‹å»ºç«‹å®Œæˆ: ResNet18\")\n",
    "print(f\"   åƒæ•¸é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   å¯è¨“ç·´åƒæ•¸: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—é¡åˆ¥æ¬Šé‡ï¼ˆè™•ç†ä¸å¹³è¡¡ï¼‰\n",
    "class_counts = train_data['Class'].value_counts().sort_index()\n",
    "total = len(train_data)\n",
    "class_weights = torch.tensor([total / (2 * class_counts[i]) for i in range(2)], dtype=torch.float32)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "print(f\"ğŸ“Š é¡åˆ¥æ¬Šé‡ï¼ˆç”¨æ–¼è™•ç†ä¸å¹³è¡¡ï¼‰:\")\n",
    "print(f\"   Class 0: {class_weights[0]:.4f}\")\n",
    "print(f\"   Class 1: {class_weights[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)  # åŠ æ¬Šæå¤±\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "print(\"âœ… è¨“ç·´è¨­å®šå®Œæˆ\")\n",
    "print(f\"   æå¤±å‡½æ•¸: CrossEntropyLoss (åŠ æ¬Š)\")\n",
    "print(f\"   å„ªåŒ–å™¨: Adam (lr=0.001)\")\n",
    "print(f\"   å­¸ç¿’ç‡èª¿åº¦: StepLR (æ¯ 10 epoch é™ç‚º 0.1 å€)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ è¨“ç·´æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´å‡½æ•¸\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), correct / total\n",
    "\n",
    "# é©—è­‰å‡½æ•¸\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())  # Class 1 çš„æ©Ÿç‡\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # è¨ˆç®—æŒ‡æ¨™\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # è™•ç† AUCï¼ˆå¦‚æœåªæœ‰ä¸€å€‹é¡åˆ¥æœƒå ±éŒ¯ï¼‰\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    return running_loss / len(loader), accuracy, auc, all_preds, all_probs, all_labels\n",
    "\n",
    "print(\"âœ… è¨“ç·´å‡½æ•¸å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é–‹å§‹è¨“ç·´\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "# è¨˜éŒ„æ­·å²\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_auc': []\n",
    "}\n",
    "\n",
    "best_auc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ é–‹å§‹è¨“ç·´\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # è¨“ç·´\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # é©—è­‰\n",
    "    val_loss, val_acc, val_auc, _, _, _ = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # æ›´æ–°å­¸ç¿’ç‡\n",
    "    scheduler.step()\n",
    "    \n",
    "    # è¨˜éŒ„\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    \n",
    "    # å„²å­˜æœ€ä½³æ¨¡å‹\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    # é¡¯ç¤ºé€²åº¦\n",
    "    print(f\"Epoch {epoch+1:02d}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… è¨“ç·´å®Œæˆï¼æœ€ä½³ AUC: {best_auc:.4f} (Epoch {best_epoch})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ è¦–è¦ºåŒ–è¨“ç·´éç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¹ªè£½è¨“ç·´æ›²ç·š\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train', marker='o', markersize=3)\n",
    "axes[0].plot(history['val_loss'], label='Validation', marker='o', markersize=3)\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train', marker='o', markersize=3)\n",
    "axes[1].plot(history['val_acc'], label='Validation', marker='o', markersize=3)\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[2].plot(history['val_auc'], label='Validation AUC', marker='o', markersize=3, color='green')\n",
    "axes[2].axhline(y=best_auc, color='r', linestyle='--', label=f'Best: {best_auc:.4f}')\n",
    "axes[2].set_title('Validation AUC')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('AUC')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"ğŸ“Š è¨“ç·´æ›²ç·šå·²å„²å­˜: training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ æœ€çµ‚è©•ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥æœ€ä½³æ¨¡å‹\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(f\"âœ… è¼‰å…¥æœ€ä½³æ¨¡å‹ (Epoch {best_epoch})\")\n",
    "\n",
    "# æœ€çµ‚é©—è­‰\n",
    "val_loss, val_acc, val_auc, all_preds, all_probs, all_labels = validate(\n",
    "    model, val_loader, criterion, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—è©³ç´°æŒ‡æ¨™\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š æœ€çµ‚è©•ä¼°çµæœ\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"âœ… Accuracy:  {val_acc:.4f} ({val_acc*100:.1f}%)\")\n",
    "print(f\"âœ… AUC-ROC:   {val_auc:.4f}\")\n",
    "print()\n",
    "\n",
    "# å„é¡åˆ¥æŒ‡æ¨™\n",
    "precision_0 = precision_score(all_labels, all_preds, pos_label=0)\n",
    "precision_1 = precision_score(all_labels, all_preds, pos_label=1)\n",
    "recall_0 = recall_score(all_labels, all_preds, pos_label=0)\n",
    "recall_1 = recall_score(all_labels, all_preds, pos_label=1)\n",
    "f1_0 = f1_score(all_labels, all_preds, pos_label=0)\n",
    "f1_1 = f1_score(all_labels, all_preds, pos_label=1)\n",
    "\n",
    "print(\"ğŸ“Š å„é¡åˆ¥æŒ‡æ¨™:\")\n",
    "print(f\"{'':15} {'Precision':>10} {'Recall':>10} {'F1-Score':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Class 0 (ä¸è‘—åºŠ)':15} {precision_0:>10.4f} {recall_0:>10.4f} {f1_0:>10.4f}\")\n",
    "print(f\"{'Class 1 (è‘—åºŠ)':15} {precision_1:>10.4f} {recall_1:>10.4f} {f1_1:>10.4f}\")\n",
    "print()\n",
    "\n",
    "# âš ï¸ é‡é»æŒ‡æ¨™\n",
    "print(\"âš ï¸ é‡é»é—œæ³¨ Class 1 (æœƒè‘—åºŠ) çš„ Recall!\")\n",
    "print(f\"   â†’ æœƒè‘—åºŠçš„èƒšèƒï¼Œæ¨¡å‹çŒœå°äº† {recall_1*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ··æ·†çŸ©é™£\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['é æ¸¬: ä¸è‘—åºŠ', 'é æ¸¬: è‘—åºŠ'],\n",
    "            yticklabels=['å¯¦éš›: ä¸è‘—åºŠ', 'å¯¦éš›: è‘—åºŠ'])\n",
    "plt.title('æ··æ·†çŸ©é™£ (Confusion Matrix)')\n",
    "plt.ylabel('å¯¦éš›å€¼')\n",
    "plt.xlabel('é æ¸¬å€¼')\n",
    "\n",
    "# æ¨™è¨»èªªæ˜\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "plt.figtext(0.5, -0.05, \n",
    "            f'TN={tn} (çŒœå°ä¸è‘—åºŠ) | FP={fp} (èª¤å ±) | FN={fn} (æ¼æ‰) | TP={tp} (çŒœå°è‘—åºŠ)', \n",
    "            ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"ğŸ“Š æ··æ·†çŸ©é™£å·²å„²å­˜: confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´åˆ†é¡å ±å‘Š\n",
    "print(\"\\nğŸ“Š å®Œæ•´åˆ†é¡å ±å‘Š:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['ä¸è‘—åºŠ', 'è‘—åºŠ']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ çµæœæ‘˜è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¢ç”Ÿçµæœæ‘˜è¦ï¼ˆæ–¹ä¾¿è¤‡è£½å›å ±ï¼‰\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“‹ EXP-001 çµæœæ‘˜è¦ï¼ˆè«‹è¤‡è£½æ­¤å€å¡Šå›å ±ï¼‰\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "## å¯¦é©—è¨­å®š\n",
    "- æ¨¡å‹: ResNet18 (é è¨“ç·´)\n",
    "- è¨“ç·´é›†: {len(train_data)} å¼µ\n",
    "- é©—è­‰é›†: {len(val_data)} å¼µ\n",
    "- Batch size: {BATCH_SIZE}\n",
    "- Epochs: {NUM_EPOCHS}\n",
    "- é¡åˆ¥åŠ æ¬Š: æ˜¯\n",
    "\n",
    "## çµæœ\n",
    "| æŒ‡æ¨™ | æ•¸å€¼ |\n",
    "|------|------|\n",
    "| Accuracy | {val_acc:.4f} ({val_acc*100:.1f}%) |\n",
    "| AUC-ROC | {val_auc:.4f} |\n",
    "| Precision (è‘—åºŠ) | {precision_1:.4f} |\n",
    "| Recall (è‘—åºŠ) | {recall_1:.4f} |\n",
    "| F1-Score (è‘—åºŠ) | {f1_1:.4f} |\n",
    "\n",
    "## æ··æ·†çŸ©é™£\n",
    "- TN (çŒœå°ä¸è‘—åºŠ): {tn}\n",
    "- FP (èª¤å ±): {fp}\n",
    "- FN (æ¼æ‰): {fn}\n",
    "- TP (çŒœå°è‘—åºŠ): {tp}\n",
    "\n",
    "## è§€å¯Ÿ\n",
    "- (è«‹å¡«å¯«ä½ çš„è§€å¯Ÿ)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè­°\n",
    "\n",
    "æ ¹æ“šçµæœï¼Œå¯ä»¥å˜—è©¦ï¼š\n",
    "\n",
    "1. **å¦‚æœ Recall å¤ªä½**ï¼š\n",
    "   - èª¿æ•´åˆ¤æ–·é–¾å€¼ï¼ˆthresholdï¼‰\n",
    "   - å¢åŠ  Class 1 çš„æ¬Šé‡\n",
    "   - å˜—è©¦ Oversampling\n",
    "\n",
    "2. **å¦‚æœéæ“¬åˆ**ï¼ˆè¨“ç·´å¥½ä½†é©—è­‰å·®ï¼‰ï¼š\n",
    "   - å¢åŠ  Dropout\n",
    "   - å¢åŠ è³‡æ–™å¢å¼·\n",
    "   - æ¸›å°‘æ¨¡å‹è¤‡é›œåº¦\n",
    "\n",
    "3. **å¦‚æœæ•´é«”æ•ˆæœä¸å¥½**ï¼š\n",
    "   - å˜—è©¦å…¶ä»–æ¨¡å‹ï¼ˆEfficientNet, ConvNeXtï¼‰\n",
    "   - å¢åŠ è¨“ç·´ epochs\n",
    "   - èª¿æ•´å­¸ç¿’ç‡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
